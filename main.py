import streamlit as st
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_groq import ChatGroq
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø©
st.set_page_config(page_title="Lara AI Study", page_icon="ğŸ“", layout="wide")

# ØªØµÙ…ÙŠÙ… Ø§Ù„ÙˆØ§Ø¬Ù‡Ø©
st.markdown("""
<style>
    @import url('https://fonts.googleapis.com/css2?family=Cairo:wght@400;700&display=swap');
    * { font-family: 'Cairo', sans-serif; direction: rtl; }
    .stApp { background-color: #ffffff; }
    .main-title { color: #6c5ce7; text-align: center; font-size: 3rem; font-weight: bold; }
    .footer { position: fixed; bottom: 0; left: 0; width: 100%; background: #6c5ce7; color: white; text-align: center; padding: 10px; font-weight: bold; }
</style>
""", unsafe_allow_html=True)

st.markdown('<p class="main-title">ğŸ“š Ù…Ù†ØµØ© Ù„Ø§Ø±Ø§ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ© Ø§Ù„Ø°ÙƒÙŠØ©</p>', unsafe_allow_html=True)

# Ø¬Ù„Ø¨ Ø§Ù„Ù…ÙØªØ§Ø­ Ù…Ù† Secrets
try:
    api_key = st.secrets["GROQ_API_KEY"]
except:
    st.error("âš ï¸ ÙŠØ±Ø¬Ù‰ Ø¶Ø¨Ø· Ø§Ù„Ù€ API Key ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Streamlit Secrets")
    st.stop()

# Ø±ÙØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª
uploaded_files = st.file_uploader("ğŸ“‚ Ø§Ø±ÙØ¹ÙŠ ÙƒØªØ¨ Ø§Ù„Ù…Ù†Ù‡Ø¬ (PDF)", accept_multiple_files=True)

if uploaded_files:
    if st.button("ğŸš€ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒØªØ¨ ÙˆØ¨Ø¯Ø¡ Ø§Ù„Ø¯Ø±Ø§Ø³Ø©"):
        with st.spinner("Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØªØ§Ø¨... Ù„Ø§Ø±Ø§ ØªØ¹Ù…Ù„ Ø¨Ø¬Ø¯ ğŸ‘©â€ğŸ’»"):
            text = ""
            for pdf in uploaded_files:
                pdf_reader = PdfReader(pdf)
                for page in pdf_reader.pages:
                    text += page.extract_text()
            
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
            chunks = text_splitter.split_text(text)
            embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
            vector_store = FAISS.from_texts(chunks, embedding=embeddings)
            st.session_state.vectors = vector_store
            st.success("ØªÙ… ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒØªØ§Ø¨ Ø¨Ù†Ø¬Ø§Ø­! ÙŠÙ…ÙƒÙ†Ø¬ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø¢Ù†.")

# Ø§Ù„Ø´Ø§Øª
query = st.text_input("â“ Ù…Ø§ Ù‡Ùˆ Ø³Ø¤Ø§Ù„Ùƒ Ù…Ù† Ø¯Ø§Ø®Ù„ Ø§Ù„ÙƒØªØ§Ø¨ØŸ")
if query and "vectors" in st.session_state:
    with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù† Ø§Ù„Ù…Ù†Ù‡Ø¬..."):
        llm = ChatGroq(groq_api_key=api_key, model_name="llama3-70b-8192")
        template = """Ø£Ù†Øª Ù…Ø¹Ù„Ù… Ù…Ø³Ø§Ø¹Ø¯ Ø®Ø¨ÙŠØ±. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø±ÙÙ‚ ÙÙ‚Ø· Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„. 
        Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©ØŒ Ù‚Ù„ Ø£Ù†Ù‡Ø§ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„ÙƒØªØ§Ø¨ Ø§Ù„Ù…Ø±ÙÙ‚.
        Ø§Ù„Ù†Øµ: {context}
        Ø§Ù„Ø³Ø¤Ø§Ù„: {question}
        Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:"""
        prompt = PromptTemplate(template=template, input_variables=["context", "question"])
        chain = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=st.session_state.vectors.as_retriever(), chain_type_kwargs={"prompt": prompt})
        
        response = chain.run(query)
        st.info(response)

# Ø§Ù„ÙÙˆØªØ± Ø§Ù„Ø«Ø§Ø¨Øª
st.markdown('<div class="footer">ØµÙ†Ø¹ Ø¨ÙƒÙ„ Ø­Ø¨ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„Ù…Ø·ÙˆØ±Ø© Ù„Ø§Ø±Ø§ â¤ï¸ 2026</div>', unsafe_allow_html=True)
